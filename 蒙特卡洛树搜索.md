### 蒙特卡洛树搜索算法笔记

##### 1.树的节点$v_i$
  
* **局面$R(v_i)$**：每个节点代表着每个不同的局面：在矩阵搜索中，**局面**就定为一个递增的数字序列，例如目前处于的节点是$[1,2]$，根据矩阵下一行的内容，接下来可选的局面可能是$[1,2,3]$或者$[1,2,4]$或者失败（没有可选局面）等等。
  * 搜索树的**根节点**就是需要进行决策的**当前局面**，我们希望蒙特卡诺树能够为我们评估**当前局面**之后的每一个**可选局面**的价值。
* **胜利次数$Q(v_i)$**：每个节点都存储有这个节点的**胜利次数**。节点的胜利次数会在一个新拓展的节点完成模拟后的反向传播阶段经过这个节点的时候更新。
* **模拟次数$N(v_i)$**：每个节点都存储有这个节点的**模拟次数**，节点的模拟次数会在一个新拓展的节点完成模拟后的反向传播阶段经过这个节点的时候更新。
![](https://raw.githubusercontent.com/Casseluse/image_for_linalg/master/20220910160524.png)
![](https://raw.githubusercontent.com/Casseluse/image_for_linalg/master/20220910164651.png)
##### 2.搜索过程

* **选择（Selection）**：每一次迭代就是从**当前节点**选择下一个**要去的节点**的过程。我们的出发点就是当前我们所处的节点$v_c$。**$v_c$有3种状态**：
  * $v_c$的子节点都已被拓展过：此时根据$UCT$公式计算所有$UCT$子节点的$UCT$值，并向$UCT$值最大的子节点**移动**。
  * $v_c$还存在未被拓展过的子节点：随机选择$v_i$的一个未被拓展的子节点**执行拓展步骤**。
  * $v_c$不存在子节点：这意味着$R(v_i)$代表着游戏结束的局面，一般包括胜利或是失败两种情况，此时，**从$v_i$执行反向传播步骤**。
* **拓展（Expansion）**：在搜索树中创建一个新的节点作为$v_c$的新子节点，记为$v_e$，初始化$[Q(v_e),N(v_e)]=[0,0]$，并向$v_e$该节点**移动**，到达$v_e$后**执行模拟步骤**。
* **模拟（Simulation）**：$v_e$是**执行拓展步骤后**产生的节点，此时从$v_e$开始，让游戏随机进行，即随机向子节点移动，并重复这个过程，直到游戏结束，一般来说，若游戏胜利，则$[Q(v_e),N(v_e)]=[1,1]$，否则为$[0,1]$。*这就是蒙特卡洛步骤吧*
* **反向传播（Back Propagation）**：在$v_e$的模拟结束之后，从$v_e$向上追溯到根节点的这条路径$path$上的所有节点都根据$v_e$本次模拟的结果更新$[Q(v_{path}),N(v_{path})]$，更新的规则是若$v_e$模拟结束后游戏胜利，则$$Q(v_{path})=Q(v_{path})+1\\ N(v_{path})=N(v_{path})+1$$若失败，则$$Q(v_{path})=Q(v_{path})\\ N(v_{path})=N(v_{path})+1$$
3. **UCT函数**
   $$UCT(v_i,v)=\frac{Q(v_i)}{N(v_i)}+c\sqrt{\frac{log(N(v))}{N(v_i)}}$$